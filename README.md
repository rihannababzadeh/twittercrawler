Documentation and Report for Data Crawling
1.	It was my first time ever creating a data scrapper. So first, I created a twitter app and I got key and token needed for this assignment. There should be hidden in the code but I have not done that yet.
2.	I created a twitter_credentials.py file to store this credential.
3.	I installed tweepy which is a Python library for accessing the Twitter API.
4.	twitter_streamer1.py is the file which retrieves the 25 first tweets from @realdonaldtrump.
5.	I could not open rss for CNN because of ERR_TOO_MANY_REDIRECTS. Even after clearing my cookies the problem was not solve. So instead of CNN, in twitter_streamer2.py I have retrieved 25 tweets about Trump in twitter. 
6.	Moreover in news.py, 25 news are retrieved from New York Times politics page. I have used Universal Feed Parser which is a Python module for downloading and parsing syndicated feeds. In addition I have used Beautiful Soup which is a Python library for getting data out of HTML and XML files.
7.	Up to this part, I faced many problems as everything was new to me. I solved them but finally I got stuck with the JSON files that I had created to store the crawled data. While I was working with JS to create my webpage and showing the data using these JSON files, there was always errors with the file. I read from the file, which some people in Stackoverflow mentioned that it is not possible and you need to have the JSON in your code. Then I put one of my JSON file records in the script. Again there was the same error. So I doubt if the file that Python is creating is really a correct JSON!
8.	I used several online JSON parsers. I passed one record and several records. There was usually errors in the file (as accuracy of these online parsers are different some of them did not show errors).
9.	My cool idea which I did not have enough time to implement a data visualization. Using the API in https://github.com/jasondavies/d3-cloud to show a simple visualized analysis of words used in trumps tweets. 
